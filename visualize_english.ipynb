{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect difficulty of English text and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyrus/virtualenv/text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertModel\n",
    "\n",
    "from models.attention import SimpleAttention\n",
    "from dataset.english_cefr_dataset import EnglishCEFRDataset\n",
    "from util.metrics import ClassificationMetrics\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20\n",
    "LOAD_FROM = None\n",
    "DATA_ROOT = Path(\"training_data\")\n",
    "NUM_CLASS = 5\n",
    "EXP_FOLDER = Path(\"exp1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EnglishCEFRDataset(DATA_ROOT / \"cefr_leveled_texts.csv\")\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.1, 0.2])\n",
    "\n",
    "train_dataset = Subset(train_dataset, np.arange(500))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = Subset(val_dataset, np.arange(20))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = SimpleAttention(NUM_CLASS, vocab_size=len(tokenizer)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # to get average easily\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hi!\\nI've been meaning to write for ages and finally today I'm actually doing something about it. Not that I'm trying to make excuses for myself, it's been really hard to sit down and write, as I've been moving around so much. Since we last saw each other I've unpacked my bags in four different cities. This job has turned out to be more of a whirlwind than I expected, but it's all good! \\nI went from London to Prague to set up a new regional office there. You know I'd always wanted to go, but maybe I was imagining Prague in spring when I used to talk about that. Winter was really hard, with minus 15 degrees in the mornings and dark really early in the evening. But at least it was blue skies and white snow and not days on end of grey skies and rain, like at home. It's tough being away from home over Christmas, though, and Skype on Christmas Day wasn't really the same as being with everyone.\\nFrom there I was on another three-month mission to oversee the set-up of the office in New York. Loved, loved, loved New York! It's like being in one big TV show, as everywhere looks just a little bit familiar. I did every tourist thing you can think of when I wasn't working, and must have spent most of my salary on eating out. It was really hard to leave for the next job, especially as I kind of met someone (!) More about Michael later ...\\nSo then I was posted to LA, which felt like a whole other country compared with the East Coast. I could definitely get used to that kind of outdoor, beach lifestyle, but I didn't spend as much time getting to know California as I could have because I was flying back to see Michael every other weekend. He came to see me when he could, but his job means he's often working at weekends, so he couldn't make the flight very often. Those three months flew by and then I was off again, to Frankfurt, which is where I am now. And … so is Michael! He got a month off work and we're trying to work out how we can be in the same place at the same time for a while. We figure the first step in that direction is getting married, which is also why I wanted to write – I can't get married without my oldest friend there! The wedding's going to be at home in London in September and I hope you can come!\\nAnyway, tell me all your news and I promise not to leave it so long this time!\\nLots of love,\\nKath</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>﻿It was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\nSo unbearable did some find it that they took up the safe but alarming opportunity to give themselves mild electric shocks in an attempt to break the tedium. \\nTwo-thirds of men pressed a button to deliver a painful jolt during a 15-minute spell of solitude. \\nUnder the same conditions, a quarter of women pressed the shock button. The difference, scientists suspect, is that men tend to be more sensation-seeking than women. \\nThe report from psychologists at Virginia and Harvard Universities is one of a surprising few to tackle the question of why most of us find it so hard to do nothing. \\nIn more than 11 separate studies, the researchers showed that people hated being left to think, regardless of their age, education, income or the amount of time they spent using smartphones or social media. \\nTimothy Wilson, who led the work, said the findings were not necessarily a reflection of the pace of modern life or the spread of mobile devices and social media. Instead, those things might be popular because of our constant urge to do something rather than nothing. \\nThe first run of experiments began with students being ushered – alone, without phones, books or anything to write with – into an unadorned room and told to think. The only rules were they had to stay seated and not fall asleep. They were informed – specifically or vaguely – that they would have six to 15 minutes alone. \\nThe students were questioned when the time was up. On average, they did not enjoy the experience. They struggled to concentrate. Their minds wandered even with nothing to distract them. Even giving them time to think about what to think about did not help. \\nIn case the unfamiliar setting hampered the ability to think, the researchers ran the experiment again with people at home. \\nThey got much the same results, only people found the experience even more miserable and cheated by getting up from their chair or checking their phones. \\nTo see if the effect was found only in students, the scientists recruited more than 100 people, aged 18 to 77, from a church and a farmers’ market. They, too, disliked being left to their thoughts. \\nBut, the most staggering result was yet to come. To check whether people might actually prefer something bad to nothing at all, the students were given the option of administering a mild electric shock. \\nThey had been asked earlier to rate how unpleasant the shocks were, alongside other options, such as looking at pictures of cockroaches or hearing the sound of a knife rubbing against a bottle. \\nAll the students picked for the test said they would pay to avoid mild electric shocks after receiving a demonstration. \\nTo the researchers’ surprise, 12 of 18 men gave themselves up to four electric shocks, as did six of 24 women. \\n“What is striking is that simply being alone with their thoughts was apparently so aversive that it drove many participants to self-administer an electric shock that they had earlier said they would pay to avoid,” the scientists write in Science  \\nJessica Andrews-Hanna at the University of Colorado said many students would probably zap themselves to cheer up a tedious lecture. But, she says more needs to be known about the motivation of the shockers in Wilson’s study. \\n“Imagine the setup – a person is told to sit in a chair with wires attached to their skin and a button that will deliver a harmless but uncomfortable shock, and they are told to just sit there and entertain themselves with their thoughts,” she said. \\n“As they sit there, strapped to this machine, their mind starts to wander and it naturally goes to that shock – was it really that bad? \\n“What are the experimenters really interested in? Perhaps this is a case where curiosity killed the cat.”</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keith recently came back from a trip to Chicago, Illinois. This midwestern metropolis is found along the shore of Lake Michigan. During his visit, Keith spent a lot of time exploring the city to visit important landmarks and monuments.\\nKeith loves baseball, and he made sure to take a visit to Wrigley Field. Not only did he take a tour of this spectacular stadium, but he also got to watch a Chicago Cubs game. In the stadium, Keith and the other fans cheered for the Cubs. Keith was happy that the Cubs won with a score of 5-4.\\nChicago has many historic places to visit. Keith found the Chicago Water Tower impressive as it is one of the few remaining landmarks to have survived the Great Chicago Fire of 1871. Keith also took a walk through Jackson Park, a great outdoor space that hosted the World’s Fair of 1892. The park is great for a leisurely stroll, and it still features some of the original architecture and replicas of monuments that were featured in the World’s Fair.\\nDuring the last part of his visit, Keith managed to climb the stairs inside of the Willis Tower, a 110-story skyscraper. Despite the challenge of climbing the many flights of stairs, Keith felt that reaching the top was worth the effort. From the rooftop, Keith received a gorgeous view of the city’s skyline with Lake Michigan in the background.</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Griffith Observatory is a planetarium, and an exhibit hall located in Los Angeles's Griffith Park. It features several astronomical displays and telescopes in the Hollywood Hills in the Los Feliz district of the city. The Observatory was built on land donated by Griffith J. Griffith in 1896. Griffith was a Welsh-born philanthropist and mining tycoon, who wanted to give the public access to a state-of-the-art observatory, so it was free of charge to visitors. He felt common people should be able to enjoy the wonders of the universe, and not be solely for the scientific community.\\nGriffith donated 3,015 acres of land specifically to house the observatory that opened on May 14, 1935. During the first five days after the opening, the planetarium averaged more than 13,000 visitors a day. One of the major attractions was the Foucault Pendulum that is designed to show the earth's rotation and the Zeiss reflecting telescope. The observatory has a striking view of Los Angeles and Hollywood, and the Pacific Ocean can be seen on clear days from there.\\nIn 2002 the planetarium was subjected to a $93 million renovation and expansion, which closed the museum for four years. After it reopened in 2006, the facility expanded underground with new exhibits, a cafe, a gift shop, and a theater. The Wilder Hall of the Eye section concentrates on the astronomical aspect of the observatory. It is where people can view the stars through the observatory's telescope, and features an interactive exhibit that is very popular. The Ahmanson Hall of the Sky is another popular attraction for viewing images in the night sky that are visible to the naked eye. The observatory frequently stages viewing nights when the public is invited to bring their own telescopes. It is open daily, and remains free of charge.</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-LRB- The Hollywood Reporter -RRB- It's official: AMC's The Walking Dead companion series has a title.\\nExecutive producer Robert Kirkman announced Friday that the companion series, which starts as a prequel to the original, will be titled Fear The Walking Dead. \\nThe news comes as the companion went through development season -- and was picked up to series and preemptively renewed for a second season -- under code names including Cobalt and Fear The Walking Dead. \\nOn March 9, AMC announced the series pickup and renewal for the then-untitled series.\\nKirkman, who created The Walking Dead comic series, co-wrote the pilot with Dave Erickson of Sons of Anarchy. Kirkman and Erickson will executive produce alongside Walking Dead's Gale Anne Hurd and Dave Alpert. Erickson will serve as showrunner, and Adam Davidson will direct the pilot. Walking Dead visual effects guru Greg Nicotero will also executive produce the series.\\nWhile AMC has been tight-lipped on the seriespremise, sources tell The Hollywood Reporter that the drama is a prequel that takes place in Los Angeles at the onset of the zombie outbreak. AMC confirmed that the companion series is set in Los Angeles but revealed nothing more beyond that it will focus on new characters and storylines.\\nCliff Curtis -LRB- Gang Related -RRB- stars as Sean Cabrera, a teacher who shares a son with his ex-wife. Sons of Anarchy's Kim Dickens is set to co-star as Nancy, a guidance counselor who works at the school with Sean and is seeing him romantically. Frank Dillane -LRB- Harry Potter and the Half-Blood Prince -RRB- co-stars as Nancy's son Nick, who has battled a drug problem. And Alycia Debnam Carey -LRB- Into the Woods -RRB- is will play Nancy's ambitious daughter, Ashley, who is the polar opposite of Nick and dreams of leaving Los Angeles for Berkeley when the apocalypse strikes.\\n Fear The Walking Dead will premiere in the late summer with season two set for 2016. An official premiere date has not yet been announced. The season five finale of the flagship series airs Sunday at 9 p.m.\\n© 2015 The Hollywood Reporter. All rights reserved.\\n</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Light propagating in the vicinity of astrophysical compact objects, like neutron stars and black holes, is affected by the gravitational field. It has been demonstrated that the general relativistic effects might be important for understanding the features of the radiation\\ncoming from the neutron star like objects [1, 2, 3]. The\\ngravitational redshift and bending of light rays emitted\\nby a compact object affect the form and spectrum of the\\nobserved signals. For the emission of light from the vicinity of a black hole these effects are more profound than\\nfor neutron stars. For example, the line broadening of Xray observed by the ASCA satellite can be explained by\\nthe strong gravitational effect on the light emitted from\\nthe accretion disk located near to the central black hole\\n[4, 5].\\nAstrophysical black holes are believed to be formed\\nas a result of the gravitational collapse of massive stars\\n[6]. Then it is natural to expect that the radiation emitted during the gravitational collapse is also affected by\\nthe strong gravity. For a spherical collapse and continuous emission of light this effect was studied in details\\n[7, 8, 9]. Recently the light curves for collapsing objects\\nwere studied in a slightly different set up [10] assuming\\nthat the radiation has a profile of a sharp in time pulse.\\nSuch radiation may occur during the collapse of a star\\nwhen its matter density becomes much higher than the\\nnuclear density. Under these conditions hadronic phase\\ntransitions are expected [11] which may result in sharpin-time emission of massless particles (photons and neutrino) [12].\\nIn this work, we consider the radiation emitted by a\\ncollapsing star during a finite time interval and calculate\\nlight curves and the spectrum of this radiation as seen by\\na distant observer. As in the previous work [10], we adopt\\na simplified model of a freely falling spherical surface and\\nassume that the radiation is originally monochromatic.\\nBut instead of instant radiation, we focus on the radiation emitted during the finite interval of time. The main\\ngoal of this study is to analyze how one can extract information about the characteristics of a collapsing object\\n(its mass and radius) from the observed spectra and light\\ncurves.</th>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future of dentistry has become one of the most unique resources and providers of this oral systemic\\napproach. In determining the patient's unique needs, it is becoming increasingly important for the\\ndentist to work with other cutting-edge, integrative practitioners to fully assess the individual\\nrequirements of the patient before, during, and after dental care. Witnessing the beginning of truly\\ngroundbreaking advances in technology is a rare opportunity. Skepticism is a natural reaction when we\\nare presented with a radically new method and its potential uses. Skepticism helps us filter the valuable\\nfrom the worthless, the permanent from the ephemeral, and the rational from the preposterous. The\\npresent article describes a brief review on some of the breakthrough advances in dentistry regarding\\ntissue engineering, nanodentistry and ozone therapy.\\nKey words: Nanodentistry, ozone therapy, tissue engineering.\\nINTRODUCTION\\nPatients today are increasingly taking more responsibility\\nfor their own health care. They tend to be well-read,\\neducated by previous health-care experiences, and much\\nmore discriminating in their own choice of practitioners\\nand treatment methods. As health practitioners struggling\\nwith the complex task of helping these patients achieve\\noptimum wellness, we should constantly search for the\\ninformation that best answers their questions and offers\\nsolutions to their health concerns (Kumar et al., 2010).\\nNanodentistry will make possible the maintenance of\\ncomprehensive oral health by employing nanomaterials,\\nbiotechnology including tissue engineering, and\\nultimately, dental nanorobotics (nanomedicine). When the\\nfirst micron-size dental nanorobots can be constructed in\\n10 to 20 years, these devices will allow precisely\\ncontrolled oral analgesia, dentition replacement therapy\\nusing biologically autologous whole replacement teeth\\nmanufactured during a single office visit, and rapid\\nnanometer-scale precision restorative dentistry. New\\ntreatment opportunities may include dentition\\nrenaturalization, permanent hypersensitivity cure,\\ncomplete orthodontic realignments during a single office\\nvisit, covalently-bonded diamondized enamel, and\\ncontinuous oral health maintenance using mechanical\\ndentifrobots (Sharma et al., 2010).\\nMost of people possess a fear towards dentistry. On\\naccount of this fear, they avoid the dental treatment.\\nInfact, people fear injections and drills that are used in\\ndental clinics. Although, in recent time, dentistry has been\\nexperiencing a period of dynamic changes and growth,\\nperhaps like no other time before. The use of ozone in\\ndental treatment is the result of this dynamics and\\ngrowth. Incorporation of ozone in dental clinic set-ups\\nwould eradicate the feeling of pain during dental\\ntreatment and also cut off the treatment time,\\nsignificantly. Ozone has been shown to stimulate\\nremineralization of recent caries-affected teeth after a\\nperiod of about six to eight weeks. Scientific support, as\\nsuggested by demonstrated studies, for ozone therapy\\npresents a potential for an atraumatic, biologically-based\\ntreatment for conditions encountered in dental practice\\n(Garg and Tandon, 2009).\\nA new direction in the field of vital pulp therapy is given\\nby the introduction of tissue engineering as an emerging\\nscience. It aims to regenerate a functional tooth-tissue\\nstructure by the interplay of three basic key elements:\\nstem cells, morphogens and scaffolds. It is a\\nmultidisciplinary approach that combines the principles of\\nbiology, medicine, and engineering to repair and/or\\nregenerate a damaged tissue and/or organ (Malhotra et \\n2 J. Dev. Biol. Tissue Eng.\\nal., 2009).\\nThe present review article describes the future of\\ndentistry and possible different treatment modalities in\\nthe future including nanodentistry, ozone therapy and\\ntissue engineering. We hope this mini review article be\\nuseful for all dentists and our colleagues and open a new\\nera in field of dentistry.</th>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>﻿The forests – and suburbs – of Europe are echoing with the growls, howls and silent padding of large predators, according to a new study that shows that brown bears, wolves and lynx are thriving on a crowded continent. Despite fears that large carnivores are doomed to extinction because of rising human populations and overconsumption, a study published in Science has found that large-predator populations are stable or rising in Europe. \\nBrown bears, wolves, the Eurasian lynx and wolverines are found in nearly one-third of mainland Europe (excluding Belarus, Ukraine and Russia), with most individuals living outside nature reserves, indicating that changing attitudes and landscape-scale conservation measures are successfully protecting species that have suffered massive persecution throughout human history. \\nBears are the most abundant large carnivore in Europe, with around 17,000 individuals, alongside 12,000 wolves, 9,000 Eurasian lynx and 1,250 wolverines, which are restricted to northern parts of Scandinavia and Finland. Only Belgium, Denmark, the Netherlands and Luxembourg in mainland Europe – like Britain – have no breeding populations of at least one large carnivore species. But the paper’s lead author and other conservationists said these animals’ surprising distribution across well-populated regions of Europe showed that even the British countryside could support big predators. \\nGuillaume Chapron from Sweden’s University of Agricultural Sciences and researchers across Europe found wolves, in some cases, living in suburban areas alongside up to 3,050 people per square kilometre – higher than the population density of Cambridge or Newcastle. On average in Europe, wolves live on land with a population density of 37 people per sq km, lynx in areas with a population density of 21 people per sq km and bears among 19 people per sq km. The population density of the Scottish Highlands is nine people per sq km. \\n“In order to have wolves, we don’t need to remove people from the landscape,” said Chapron. According to Chapron and his colleagues, the big-carnivore revival shows the success of a “land-sharing” model of conservation – in stark contrast to keeping predators and people apart by fencing off “wilderness” areas, as occurs in North America and Africa. \\n“I’m not saying it’s a peace-and-love story – coexistence often means conflict – but it’s important to manage that conflict, keep it at a low level and resolve the problems it causes. Wolves can be difficult neighbours,” said Chapron. “We shouldn’t be talking about people-predator conflict; we have conflict between people about predators. These animals are symbolic of difficult questions about how we should use the land.” \\nAccording to the researchers, this “land-sharing” approach could be applied elsewhere in the world. The reasons for its success in Europe include political stability, burgeoning populations of prey species such as wild deer and financial support for non-lethal livestock protection such as electric fences, which mean that farmers do not resort to shooting wild predators. Most crucial, said Chapron, has been the EU Habitats Directive, which has compelled member states to protect and revive rare species. \\n“Without the Habitats Directive, I don’t think we would have had this recovery,” he said. “It shows, if people are willing to protect nature and if political will is translated into strong legislation like the Habitats Directive, it’s possible to achieve results in wildlife protection.” The revival was welcomed by author and commentator George Monbiot, who is launching Rewilding Britain, a new charity to encourage the return of wild landscape and extinct species. \\n“It is great to see the upward trend continuing but Britain is completely anomalous – we’ve lost more of our large mammals than any country except for Ireland,” he said. “Apart from the accidental reintroduction of boar, we’ve done almost nothing, whereas, in much of the rest of Europe, we’ve got bears, lynx and wolves coming back. It’s a massive turnaround from the centuries of persecution.” \\nThe survey found the Eurasian lynx living permanently in 11 population groups across 23 European countries, of which only five were native populations, indicating the success of reintroduction efforts. According to Monbiot, momentum is building for the reintroduction of the lynx into the Cairngorms in Scotland. \\n“If it works in the rest of Europe, there’s absolutely no reason why it can’t work in the UK,” he said, pointing out that bears and wolves are found within an hour of Rome. “There’s no demographic reason why we can’t have a similar return of wildlife in the UK.”</th>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hedge funds are turning bullish on oil once again, betting the pandemic and investors’ environmental focus has severely damaged companies’ ability to ramp up production.\\nThe view is a reversal for hedge funds, which shorted the oil sector in the lead-up to global shutdowns, landing energy focused hedge funds gains of 26.8% in 2020, according to data from eVestment. By virtue of their fast-moving strategies, hedge funds are quick to spot new trends.\\nGlobal oil benchmark Brent has jumped 59% since early November when news of successful vaccines emerged, after COVID-19 travel curbs and lockdowns last year hammered fuel demand and collapsed oil prices. Last week it hit pre-pandemic levels close to $60 a barrel.\\nU.S. crude has climbed 54% to around $57 per barrel during the same period.\\n“By the summer, the vaccine should be widely provided and just in time for summer travel and I think things are going to go gangbusters,” said David D. Tawil, co-founder at New York-based event-driven hedge fund, Maglan Capital, and interim CEO of Centaurus Energy.\\nTawil predicted prices of $70 to $80 a barrel for Brent by the end of 2021 and is investing long independent oil and gas producers.\\nHedge funds’ bullish bets come despite the International Energy Agency warning in January a spike in new coronavirus cases will hamper oil demand this year, and a slow economic recovery would delay a full rebound in world energy demand to 2025.\\nNormally, oil producers would ramp up production as prices increase, but a move by environmentally focused investors from fossil fuels to renewables and caution by lenders leaves them hard-pressed to respond, hedge funds and other investors say.\\nThe pace of output recovery in the United States, the world’s No. 1 oil producer, is forecast to be slow and will not top its 2019 record of 12.25 million barrels per day (bpd) until 2023. Production in 2020 tumbled 6.4% to 11.47 million bpd.\\nThe Organization of the Petroleum Exporting Countries, which has also revised down demand growth, however, still expects output cuts to keep the market in deficit throughout 2021.\\nGlobal crude and condensate production was down 8% in December from February 2020, prior to the pandemic’s spread accelerating, according to Rystad Energy.\\nNorth America’s output was down 9.5% and Europe’s production declined just 1% over the same time period.\\nU.S. sanctions against Venezuela and declining oilfields in Mexico have kept oil output from Latin America sluggish.\\nSome banks are forecasting the United States, which leads with the number of COVID-19 cases, to reach herd immunity by July, which would greatly stimulate oil demand, said Jean-Louis Le Mee, head of London-based hedge fund Westbeck Capital Management, which is long a mix of oil futures and equities.\\n“Oil companies, for the first time in a long time, are likely to make a big comeback,” he said. “We have all the ingredients for an extraordinary bull market in oil for the next few years.”\\nIn the United States, hedge funds increased their allocation to Exxon Mobil Corp by 21,314 shares in the third quarter, the most recent U.S. filings compiled by Symmetric.io showed.\\nHedge funds added another 9,070 shares of U.S. majors ConocoPhillips and 4,144 to Chevron Corp over the same time period.\\nElsewhere, shorting activity in BP PLC fell by 16 million shares on Feb. 4 but increased slightly in European oil major Royal Dutch Shell Plc by 1.9 million shares, data from FIS’ Astec Analytics showed.\\nSome investors remain skeptical on Canadian oil companies, among the world’s most carbon-intensive producers, though they are bouncing back faster from the pandemic than the United States.\\nCurrent short positions rose in 10 out of 14 Canadian oil companies in the Toronto energy index during the second two weeks of January, according to filings reviewed by Reuters.\\nU.S. shale production will not quickly rebound, given the capital required and debt producers are carrying, lending oil prices support, said Rafi Tahmazian, senior portfolio manager at Calgary-based Canoe Financial LP.\\nNorth America’s oilfield services sector, which producers rely on to drill new wells, has been decimated, he said.\\n“They’re decapitated from being able to grow,” Tahmazian said. “The supply side is broken.”</th>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Without additional heating, radiative cooling of gas in the halos of massive galaxies (Milky Way and above) produces cold gas or stars in excess of that observed. Previous work suggested that AGN jets are likely required, but the form of jet energy required to quench remains unclear. This is particularly challenging for galaxy simulations, in which the resolution is orders of magnitude coarser than necessary to form and evolve the jet. On such scales, the uncertain parameters include: jet energy form (kinetic, thermal, and cosmic ray (CR) energy), energy, momentum, and mass flux, magnetic field strength and geometry, jet precession angle and period, opening-angle, and duty cycle. We investigate all of these parameters in a 1014M⊙ halo using high-resolution non-cosmological MHD simulations with the FIRE-2 (Feedback In Realistic Environments) stellar feedback model, conduction, and viscosity. We explore which scenarios match observational constraints and show that CR-dominated jets can most efficiently quench the central galaxy through a combination of CR pressure support and a modification of the thermal instability. Jets with most energy in mildly relativistic (∼ MeV or ∼1010 K) thermal plasma work, but require a factor ∼10 larger energy input. For a fixed energy flux, jets with higher specific energy (longer cooling times) quench more effectively. For this halo size, kinetic jets are less efficient in quenching unless they have wide opening or precession angles. Magnetic fields play a minor role except when the magnetic flux reaches ≳1044 erg s−1 in a kinetic jet model, which causes the jet cocoon to significantly widen, and the quenching to become explosive. We conclude that the criteria for a successful jet model are an optimal energy flux and a sufficiently wide jet cocoon with long enough cooling time at the cooling radius.</th>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   label\n",
       "text                                                    \n",
       "Hi!\\nI've been meaning to write for ages and fi...    B2\n",
       "﻿It was not so much how hard people found the c...    B2\n",
       "Keith recently came back from a trip to Chicago...    B2\n",
       "The Griffith Observatory is a planetarium, and ...    B2\n",
       "-LRB- The Hollywood Reporter -RRB- It's officia...    B2\n",
       "...                                                  ...\n",
       "Light propagating in the vicinity of astrophysi...    C2\n",
       "Future of dentistry has become one of the most ...    C2\n",
       "﻿The forests – and suburbs – of Europe are echo...    C2\n",
       "Hedge funds are turning bullish on oil once aga...    C2\n",
       "Without additional heating, radiative cooling o...    C2\n",
       "\n",
       "[1494 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mNUM_EPOCHS\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (sentences, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      6\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(sentences, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      7\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(labels)\u001b[39m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/_Project/JLPT-Difficulty-Predictor/dataset/english_cefr_dataset.py:16\u001b[0m, in \u001b[0;36mEnglishCEFRDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[0;32m---> 16\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m][index]\n\u001b[1;32m     17\u001b[0m     level \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m][index]\n\u001b[1;32m     18\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevel2target(level)\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/virtualenv/text/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (sentences, labels) in enumerate(train_loader):\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        labels = torch.LongTensor(labels).to(DEVICE)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"[Batch {batch_idx:4d}/{len(train_loader)}] Loss: {loss.item()/BATCH_SIZE:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_truths = []\n",
    "    val_outputs = []\n",
    "    for batch_idx, (sentences, labels) in enumerate(val_loader):\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        labels = torch.LongTensor(labels).to(DEVICE)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_truths += labels.detach().cpu().tolist()\n",
    "        val_outputs.append(outputs.detach().cpu())\n",
    "\n",
    "    print(f\"Total loss: {val_loss/len(val_dataset):.4f}\")\n",
    "\n",
    "    val_outputs = torch.cat(val_outputs)  # from list of tensor to numpy array\n",
    "    metrics = ClassificationMetrics(val_truths, val_outputs)\n",
    "    metrics.print_report()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 42, 42])\n",
      "tensor(16.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "          [CLS]              そ            ##ん            ##な          [UNK]          [UNK]              、          [UNK]          [UNK]              の              王          [UNK]              、          [UNK]          [UNK]              と              同          [UNK]          [UNK]              に          [UNK]              す            ##る            ##ラ            ##イ            ##オ            ##ン            ##の          [UNK]              や          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]              さ            ##な            ##い              。          [SEP]\n",
      "         0.0225         0.0022         0.0179         0.0128         0.0170         0.0151         0.0012         0.0402         0.0377         0.0182         0.0214         0.0254         0.0012         0.0328         0.0321         0.0055         0.0086         0.0360         0.0349         0.0046         0.0996         0.0376         0.0049         0.0118         0.0113         0.0097         0.0155         0.0086         0.0333         0.0438         0.0404         0.0392         0.0381         0.0371         0.0361         0.0351         0.0334         0.0120         0.0188         0.0024         0.0057         0.0382\n",
      "          [CLS]          [UNK]          [UNK]          [UNK]          [UNK]              す            ##る          [UNK]          [UNK]              の          [UNK]          [UNK]          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0958         0.0416         0.0414         0.0414         0.0418         0.0199         0.0112         0.0921         0.0981         0.0498         0.1651         0.1691         0.1327         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]              、              宿          [UNK]          [UNK]              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0999         0.0575         0.0558         0.0546         0.0540         0.0537         0.0535         0.0529         0.0507         0.0127         0.1183         0.0996         0.0948         0.0428         0.0989         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]          [UNK]              「          [UNK]          [UNK]          [UNK]              。              」          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.1366         0.1281         0.1277         0.0360         0.0870         0.0870         0.0844         0.0502         0.0939         0.1691         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]              「          [UNK]              」          [UNK]          [UNK]              き            ##ま            ##し            ##た              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0652         0.0701         0.0389         0.0518         0.0818         0.0686         0.0652         0.0915         0.0848         0.1100         0.1018         0.0748         0.0954         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]          [UNK]          [UNK]          [UNK]              か          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]              、              も            ##し              本          [UNK]          [UNK]          [UNK]          [UNK]              、          [UNK]              方          [UNK]              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0522         0.0445         0.0438         0.0432         0.0426         0.0187         0.0534         0.0532         0.0531         0.0524         0.0491         0.0060         0.0386         0.0973         0.0405         0.0337         0.0336         0.0328         0.0308         0.0028         0.0411         0.0222         0.0482         0.0135         0.0528         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]              （          [UNK]          [UNK]              ）          [UNK]          [UNK]              （          [UNK]              ）              す            ##る              ：          [UNK]              せ            ##る          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.1127         0.0396         0.0779         0.0754         0.0212         0.0952         0.0950         0.0510         0.0797         0.0169         0.0314         0.0120         0.0162         0.0997         0.0681         0.0169         0.0913         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.3482         0.1788         0.0273         0.4456         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]              一          [UNK]              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.1866         0.1776         0.0978         0.1681         0.1226         0.2474         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]              の          [UNK]          [UNK]              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.2923         0.1232         0.1705         0.1519         0.0177         0.2444         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]              野          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0943         0.0507         0.1001         0.0994         0.0984         0.0988         0.1011         0.1051         0.1255         0.1267         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]              「          [UNK]          [UNK]              」          [UNK]          [UNK]          [UNK]          [UNK]              ，          [UNK]          [UNK]              （              平              日              ）          [UNK]              ，          [UNK]          [UNK]              （              土              日              ）          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0304         0.0055         0.0236         0.0244         0.1084         0.0486         0.0465         0.0456         0.0462         0.0971         0.0479         0.0465         0.0356         0.0374         0.0105         0.0115         0.0361         0.0651         0.0422         0.0411         0.0357         0.0610         0.0118         0.0122         0.0293         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]          [UNK]              く            ##な            ##く            ##て              、          [UNK]          [UNK]          [UNK]          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.1215         0.0835         0.0830         0.0616         0.0572         0.0439         0.0172         0.0125         0.1276         0.1261         0.1320         0.1338         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]              2              .          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.3470         0.0206         0.0080         0.6245         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]          [UNK]              、          [UNK]              の            ##こ            ##と            ##か              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0914         0.1260         0.0266         0.2320         0.1445         0.0993         0.0410         0.0327         0.0469         0.1596         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n",
      "          [CLS]              大          [UNK]              な            ##の            ##は              、          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]          [UNK]              人          [UNK]          [UNK]              、          [UNK]          [UNK]          [UNK]          [UNK]              の          [UNK]          [UNK]          [UNK]              く            ##こ            ##と              。          [SEP]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]          [PAD]\n",
      "         0.0309         0.0371         0.0451         0.0475         0.0135         0.0159         0.0043         0.0360         0.0374         0.0382         0.0387         0.0391         0.0392         0.0213         0.0289         0.0278         0.0034         0.0419         0.0440         0.0447         0.0443         0.0166         0.0490         0.0500         0.0505         0.0485         0.0207         0.0366         0.0067         0.0424         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000         0.0000\n"
     ]
    }
   ],
   "source": [
    "batch_idx, (sentences, labels) = next(enumerate(train_loader))\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "tokens_list = [tokenizer.convert_ids_to_tokens(ids) for ids in inputs[\"input_ids\"]]\n",
    "\n",
    "model.eval()\n",
    "_, attention = model.get_attention_output(**inputs)\n",
    "cls_attn = attention[:, 0, :]\n",
    "print(attention.shape)\n",
    "print(cls_attn.sum())\n",
    "\n",
    "for tokens, attn in zip(tokens_list, cls_attn):\n",
    "    for t in tokens:\n",
    "        print(f\"{t:>15}\", end=\"\")\n",
    "    print()\n",
    "    for a in attn.tolist():\n",
    "        print(f\"{a:15.4f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fprop = fm.FontProperties(fname=\"resource/NotoSansJP-Regular.ttf\")\n",
    "\n",
    "i = 4\n",
    "max_length = 12\n",
    "\n",
    "tokens: list[str] = tokens_list[i]\n",
    "attn = cls_attn[i].tolist()\n",
    "label = TextDataset.target2name(labels[i].item())\n",
    "\n",
    "words = []\n",
    "values = []\n",
    "for t, a in zip(tokens, attn):\n",
    "    if t == \"[PAD]\":\n",
    "        break\n",
    "    if t.startswith(\"##\"):\n",
    "        # combine subwords\n",
    "        words[-1] = words[-1] + t[2:]\n",
    "        values[-1] += a\n",
    "    else:\n",
    "        words.append(t)\n",
    "        values.append(a)\n",
    "\n",
    "words = [\"LABEL:\"] + words\n",
    "attn_strs = [label] + [f\"{a:.4f}\" for a in values]\n",
    "colors = [\"w\"] + [str(1 - a) for a in values]\n",
    "\n",
    "\n",
    "def plot_pairs(plt, words, values, colors, max_length=max_length):\n",
    "    seq_length = len(words)\n",
    "    if seq_length < max_length:\n",
    "        words += [\"\"] * (max_length - seq_length)\n",
    "        values += [\"\"] * (max_length - seq_length)\n",
    "        colors += [\"w\"] * (max_length - seq_length)\n",
    "    plt.axis(\"off\")\n",
    "    table = plt.table(cellText=[values], cellColours=[colors], colLabels=words, loc=\"center\")\n",
    "    for cell in table._cells:\n",
    "        table._cells[cell].set_text_props(fontproperties=fprop)\n",
    "    table.scale(1, 4)\n",
    "\n",
    "\n",
    "num_lines = math.ceil(len(words) / max_length)\n",
    "fig, ax = plt.subplots(num_lines, 1, figsize=(20, 2 * num_lines))\n",
    "if num_lines == 1:\n",
    "    plot_pairs(ax, words, attn_strs, colors)\n",
    "else:\n",
    "    for i in range(num_lines):\n",
    "        s, e = i * max_length, (i + 1) * max_length\n",
    "        plot_pairs(ax[i], words[s:e], attn_strs[s:e], colors[s:e])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]], device='cuda:0')\n",
      "['[CLS]', 'Rep', '##lace', 'me', 'by', 'any', 'text', 'you', \"'\", 'd', 'like', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'Some', 'weights', 'of', 'the', 'model', 'check', '##point', 'at', 'be', '##rt', '-', 'base', '-', 'case', '##d', 'were', 'not', 'used', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "line = \"Replace me by any text you'd like.\"\n",
    "line2 = \"Some weights of the model checkpoint at bert-base-cased were not used\"\n",
    "inputs = tokenizer([line, line2], return_tensors=\"pt\", padding=\"max_length\").to(DEVICE)\n",
    "print(inputs.keys())\n",
    "print(inputs[\"attention_mask\"] == 0)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][1]))\n",
    "\n",
    "out = model(**inputs)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

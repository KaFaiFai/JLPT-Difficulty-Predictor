{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyrus/virtualenv/text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from models.bert import BERTClassification\n",
    "from models.attention import SimpleAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df_path):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(df_path, index_col=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        sentence = self.df[\"Sentence\"][index]\n",
    "        level = self.df[\"Level\"][index]\n",
    "        label = self.level2label(level)\n",
    "\n",
    "        return sentence, label\n",
    "\n",
    "    @classmethod\n",
    "    def level2label(cls, level):\n",
    "        return {\"N1\": 0, \"N2\": 1, \"N3\": 2, \"N4\": 3, \"N5\": 4}[level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1000\n",
    "NUM_WORKERS = 2\n",
    "LOAD_FROM = None\n",
    "DATA_ROOT = r\"util/jlpt_sentences.csv\"\n",
    "NUM_CLASS = 5\n",
    "EXP_FOLDER = \"exp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "dataset = TextDataset(DATA_ROOT)\n",
    "# sub_dataset = Subset(\n",
    "#     dataset, np.linspace(0, len(dataset), num=50, endpoint=False, dtype=int)\n",
    "# )\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleAttention(num_class=NUM_CLASS, vocab_size=len(tokenizer)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # to get average easily\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000]\n",
      "[Batch    1/216] Loss: 1.5872 Labels: [3, 2, 3, 3, 4, 3, 1, 0, 3, 2, 2, 0, 2, 4, 2, 0]\n",
      "[Batch   51/216] Loss: 1.5186 Labels: [1, 1, 0, 0, 2, 3, 1, 4, 0, 2, 0, 1, 3, 4, 2, 1]\n",
      "[Batch  101/216] Loss: 1.1880 Labels: [3, 0, 3, 2, 4, 2, 1, 1, 4, 2, 1, 1, 1, 2, 0, 2]\n",
      "[Batch  151/216] Loss: 0.8503 Labels: [3, 1, 2, 2, 1, 4, 2, 1, 1, 2, 2, 4, 4, 2, 1, 2]\n",
      "[Batch  201/216] Loss: 1.1127 Labels: [4, 2, 3, 3, 4, 1, 3, 4, 0, 1, 0, 0, 2, 2, 2, 3]\n",
      "Total loss: 1.3002\n",
      "Epoch [1/1000]\n",
      "[Batch    1/216] Loss: 0.9325 Labels: [1, 4, 0, 3, 3, 4, 0, 2, 2, 2, 0, 4, 4, 1, 0, 2]\n",
      "[Batch   51/216] Loss: 0.9505 Labels: [2, 3, 0, 3, 3, 4, 3, 0, 3, 4, 3, 3, 2, 2, 4, 2]\n",
      "[Batch  101/216] Loss: 0.9808 Labels: [4, 2, 1, 2, 4, 2, 1, 2, 2, 2, 4, 4, 2, 4, 3, 3]\n",
      "[Batch  151/216] Loss: 0.4852 Labels: [2, 4, 1, 1, 2, 1, 1, 2, 1, 3, 1, 0, 2, 4, 1, 3]\n",
      "[Batch  201/216] Loss: 1.3344 Labels: [1, 0, 1, 4, 2, 4, 3, 1, 4, 1, 1, 1, 3, 0, 3, 2]\n",
      "Epoch [2/1000]\n",
      "[Batch    1/216] Loss: 0.6409 Labels: [0, 3, 1, 2, 2, 4, 2, 3, 1, 1, 1, 0, 1, 4, 0, 4]\n",
      "[Batch   51/216] Loss: 0.4835 Labels: [0, 3, 4, 4, 4, 4, 3, 2, 0, 2, 4, 1, 0, 4, 3, 4]\n",
      "[Batch  101/216] Loss: 0.4227 Labels: [4, 2, 3, 2, 2, 4, 3, 0, 1, 2, 1, 3, 4, 2, 4, 1]\n",
      "[Batch  151/216] Loss: 0.6300 Labels: [1, 2, 1, 2, 0, 4, 0, 3, 3, 4, 2, 2, 4, 3, 1, 1]\n",
      "[Batch  201/216] Loss: 0.8847 Labels: [0, 1, 1, 0, 3, 3, 1, 0, 1, 1, 2, 1, 3, 2, 2, 2]\n",
      "Epoch [3/1000]\n",
      "[Batch    1/216] Loss: 0.3372 Labels: [2, 1, 2, 2, 2, 4, 4, 1, 0, 2, 2, 3, 1, 4, 2, 4]\n",
      "[Batch   51/216] Loss: 0.1862 Labels: [0, 2, 1, 1, 3, 1, 4, 1, 4, 0, 2, 1, 0, 4, 2, 4]\n",
      "[Batch  101/216] Loss: 0.1689 Labels: [0, 1, 3, 2, 2, 2, 0, 2, 1, 1, 2, 1, 3, 2, 0, 1]\n",
      "[Batch  151/216] Loss: 0.7740 Labels: [0, 3, 2, 4, 1, 2, 4, 3, 0, 3, 4, 4, 1, 4, 0, 1]\n",
      "[Batch  201/216] Loss: 0.2165 Labels: [1, 0, 2, 1, 1, 3, 1, 1, 3, 2, 4, 4, 1, 4, 3, 3]\n",
      "Epoch [4/1000]\n",
      "[Batch    1/216] Loss: 0.1517 Labels: [0, 2, 1, 4, 0, 3, 2, 4, 1, 3, 4, 2, 1, 1, 1, 2]\n",
      "[Batch   51/216] Loss: 0.1528 Labels: [0, 4, 0, 1, 0, 3, 0, 4, 4, 1, 3, 0, 0, 1, 4, 2]\n",
      "[Batch  101/216] Loss: 0.0431 Labels: [2, 1, 1, 3, 1, 3, 2, 1, 2, 3, 4, 2, 1, 4, 3, 2]\n",
      "[Batch  151/216] Loss: 0.1201 Labels: [4, 1, 0, 2, 1, 3, 1, 1, 4, 2, 0, 1, 2, 2, 2, 2]\n",
      "[Batch  201/216] Loss: 0.2122 Labels: [1, 4, 2, 1, 2, 2, 2, 2, 3, 3, 2, 2, 4, 3, 0, 4]\n",
      "Epoch [5/1000]\n",
      "[Batch    1/216] Loss: 0.1244 Labels: [0, 0, 2, 1, 1, 4, 4, 4, 2, 1, 2, 3, 1, 0, 2, 3]\n",
      "[Batch   51/216] Loss: 0.0189 Labels: [2, 1, 0, 0, 3, 4, 3, 4, 4, 2, 1, 1, 1, 2, 3, 4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     22\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Batch \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m4d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data_loader)\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m/\u001b[39mBATCH_SIZE\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Labels: \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")\n",
    "    total_loss = 0\n",
    "    all_truths = []\n",
    "    all_outputs = []\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (sentences, labels) in enumerate(data_loader):\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        labels = torch.LongTensor(labels).to(DEVICE)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\n",
    "                f\"[Batch {batch_idx+1:4d}/{len(data_loader)}]\"\n",
    "                f\" Loss: {loss.item()/BATCH_SIZE:.4f}\"\n",
    "                f\" Labels: {labels.tolist()}\"\n",
    "            )\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Total loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Epoch    1| Loss: 1.6502\n",
    "Epoch   51| Loss: 0.5745\n",
    "Epoch  101| Loss: 0.2626\n",
    "Epoch  151| Loss: 0.1694\n",
    "Epoch  201| Loss: 0.1005\n",
    "Epoch  251| Loss: 0.0651\n",
    "Epoch  301| Loss: 0.0441\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

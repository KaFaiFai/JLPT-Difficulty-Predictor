{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吾輩', 'は', '猫', 'で', 'ある', '。']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.A\n",
    "[m.surface() for m in tokenizer_obj.tokenize(\"吾輩は猫である。\", mode)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyrus/virtualenv/text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "for param in bertjapanese.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(type(bertjapanese))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  7184, 30046,     9,  6040,    12,    31,     8,     3],\n",
      "        [    2,  1429,  9590,     3,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n",
      "[CLS] 吾輩 は 猫 で ある 。 [SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1348,  0.1240,  0.1502,  ...,  0.0951,  0.2145, -0.1121],\n",
       "         [ 0.2445,  1.2341, -0.5909,  ...,  0.4571,  0.3980, -0.0478],\n",
       "         [ 0.1209,  0.3673, -0.3961,  ...,  1.0547,  0.4066,  0.2586],\n",
       "         ...,\n",
       "         [ 0.8519, -0.0191, -0.1027,  ...,  0.5163,  0.2025,  0.0164],\n",
       "         [ 0.3057,  0.0587,  0.1815,  ...,  0.9653,  0.3287,  0.8934],\n",
       "         [ 0.2922,  0.0505,  0.1648,  ...,  0.9797,  0.3315,  0.8997]],\n",
       "\n",
       "        [[ 0.2931,  0.1387,  0.5505,  ...,  0.1190,  0.1935,  0.1583],\n",
       "         [ 0.4870,  0.8291,  0.8820,  ...,  0.1859,  0.5422,  0.2236],\n",
       "         [ 0.7640,  0.6328,  0.0564,  ...,  0.6765, -0.0677,  0.5307],\n",
       "         ...,\n",
       "         [ 0.1934,  0.4562,  0.4071,  ...,  0.2353,  0.0601,  0.0404],\n",
       "         [ 0.2509,  0.5555,  0.4436,  ...,  0.1857,  0.1409, -0.0102],\n",
       "         [ 0.2529,  0.5687,  0.4067,  ...,  0.1438,  0.1298, -0.0423]]]), pooler_output=tensor([[-0.0351,  0.9865,  0.1681,  ..., -0.0908, -0.3813,  0.3728],\n",
       "        [ 0.3702,  0.9503, -0.3221,  ..., -0.1240,  0.3346,  0.1774]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input Japanese Text\n",
    "lines = [\"吾輩は猫である。\",\"国家公務員\"]\n",
    "\n",
    "inputs = tokenizer(lines, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "\n",
    "outputs = bertjapanese(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([2, 9])\n",
      "torch.Size([2, 9, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(bertjapanese, torch.nn.Module))\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "print(outputs.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyrus/virtualenv/text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from models.bert import BERTClassification\n",
    "from models.attention import SimpleAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df_path):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(df_path, index_col=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[str, int]:\n",
    "        sentence = self.df[\"Sentence\"][index]\n",
    "        level = self.df[\"Level\"][index]\n",
    "        label = self.level2label(level)\n",
    "\n",
    "        return sentence, label\n",
    "\n",
    "    @classmethod\n",
    "    def level2label(cls, level):\n",
    "        return {\"N1\": 0, \"N2\": 1, \"N3\": 2, \"N4\": 3, \"N5\": 4}[level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "NUM_WORKERS = 2\n",
    "LOAD_FROM = None\n",
    "DATA_ROOT = r\"training_data/train.csv\"\n",
    "NUM_CLASS = 5\n",
    "EXP_FOLDER = \"exp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "dataset = TextDataset(DATA_ROOT)\n",
    "# sub_dataset = Subset(\n",
    "#     dataset, np.linspace(0, len(dataset), num=50, endpoint=False, dtype=int)\n",
    "# )\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleAttention(num_class=NUM_CLASS, vocab_size=len(tokenizer)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # to get average easily\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/3]\n",
      "[Batch    1/130] Loss: 1.6061 Labels: [2, 2, 4, 1, 4, 0, 4, 2, 3, 3, 2, 2, 3, 3, 3, 2]\n",
      "[Batch   51/130] Loss: 1.3689 Labels: [3, 1, 0, 3, 2, 2, 2, 4, 2, 1, 1, 4, 2, 4, 1, 2]\n",
      "[Batch  101/130] Loss: 1.2640 Labels: [2, 4, 2, 0, 2, 2, 2, 1, 3, 3, 2, 3, 3, 3, 3, 3]\n",
      "Total loss: 1.3171\n",
      "Epoch [1/3]\n",
      "[Batch    1/130] Loss: 1.0874 Labels: [2, 2, 4, 2, 1, 3, 0, 0, 4, 3, 2, 1, 2, 1, 2, 0]\n",
      "[Batch   51/130] Loss: 0.6830 Labels: [0, 4, 4, 3, 4, 4, 2, 1, 3, 1, 2, 4, 2, 2, 1, 2]\n",
      "[Batch  101/130] Loss: 1.0881 Labels: [0, 2, 2, 4, 3, 2, 0, 3, 2, 4, 3, 0, 0, 3, 3, 1]\n",
      "Epoch [2/3]\n",
      "[Batch    1/130] Loss: 0.6145 Labels: [1, 1, 0, 2, 0, 3, 0, 4, 2, 3, 0, 2, 1, 4, 2, 4]\n",
      "[Batch   51/130] Loss: 0.5474 Labels: [2, 3, 2, 2, 0, 1, 0, 3, 4, 0, 4, 3, 2, 4, 2, 1]\n",
      "[Batch  101/130] Loss: 0.5072 Labels: [2, 1, 2, 0, 3, 2, 2, 2, 0, 0, 0, 3, 1, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")\n",
    "    total_loss = 0\n",
    "    all_truths = []\n",
    "    all_outputs = []\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (sentences, labels) in enumerate(data_loader):\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        labels = torch.LongTensor(labels).to(DEVICE)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\n",
    "                f\"[Batch {batch_idx+1:4d}/{len(data_loader)}]\"\n",
    "                f\" Loss: {loss.item()/BATCH_SIZE:.4f}\"\n",
    "                f\" Labels: {labels.tolist()}\"\n",
    "            )\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Total loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEpoch    1| Loss: 1.6502\\nEpoch   51| Loss: 0.5745\\nEpoch  101| Loss: 0.2626\\nEpoch  151| Loss: 0.1694\\nEpoch  201| Loss: 0.1005\\nEpoch  251| Loss: 0.0651\\nEpoch  301| Loss: 0.0441\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Epoch    1| Loss: 1.6502\n",
    "Epoch   51| Loss: 0.5745\n",
    "Epoch  101| Loss: 0.2626\n",
    "Epoch  151| Loss: 0.1694\n",
    "Epoch  201| Loss: 0.1005\n",
    "Epoch  251| Loss: 0.0651\n",
    "Epoch  301| Loss: 0.0441\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokens_list \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(ids) \u001b[39mfor\u001b[39;00m ids \u001b[39min\u001b[39;00m inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 6\u001b[0m _, attention \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m      7\u001b[0m cls_attn \u001b[39m=\u001b[39m attention[:, \u001b[39m0\u001b[39m, :]\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(attention\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "batch_idx, (sentences, labels) = next(enumerate(data_loader))\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "tokens_list = [tokenizer.convert_ids_to_tokens(ids) for ids in inputs[\"input_ids\"]]\n",
    "\n",
    "model.eval()\n",
    "_, attention = model.get_attention_output(**inputs)\n",
    "cls_attn = attention[:, 0, :]\n",
    "print(attention.shape)\n",
    "print(cls_attn.sum())\n",
    "\n",
    "for tokens, attn in zip(tokens_list, cls_attn):\n",
    "    for t in tokens:\n",
    "        print(f\"{t:>15}\", end=\"\")\n",
    "    print()\n",
    "    for a in attn.tolist():\n",
    "        print(f\"{a:15.4f}\", end=\"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
